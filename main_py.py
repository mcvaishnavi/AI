# -*- coding: utf-8 -*-
"""main.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16In3kcr8L2DoGds-KAdDaCrCDUswoI8i
"""

import pandas as pd
from tqdm import tqdm
from nltk.stem import WordNetLemmatizer
import nltk
from bs4 import BeautifulSoup
import re
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

data = pd.read_csv("drive/MyDrive/data/train.txt",header=None, sep=";", names=["Comment","Emotion"], encoding="utf-8")
data.head()

print("Number of rows : ",data.shape[0])
print("Number of columns : ", data.shape[1])

data.info()

data.isnull().sum()

import seaborn as sns

sns.countplot(data['Emotion'])

from wordcloud import WordCloud

def words_cloud(wordcloud, df):
    plt.figure(figsize=(10, 10))
    plt.title(df+' Word Cloud', size = 16)
    plt.imshow(wordcloud) 
    plt.axis("off");

emotions_list = data['Emotion'].unique()
emotions_list

for emotion in emotions_list:
    text = ' '.join([sentence for sentence in data.loc[data['Emotion'] == emotion,'Comment']])
    wordcloud = WordCloud(width = 600, height = 600).generate(text)
    words_cloud(wordcloud, emotion)

"""Data cleaning and preprocessing"""

text = data["Comment"][1]
text

cleantext = BeautifulSoup(text, "lxml").text
cleantext

cleantext = re.sub(r'[^\w\s]','',cleantext)
cleantext

nltk.download('stopwords')

print(stopwords.words('english'))

cleantext= cleantext.lower()
stopword = set(stopwords.words('english'))
tokens= cleantext.split()
token_list = []
for token in tokens:
    if token not in stopword:
        token_list.append(token)

stopword = set(stopwords.words('english'))

token_list

lemmatizer = WordNetLemmatizer()

import nltk
nltk.download('omw-1.4')
nltk.download('wordnet')

lemmatizer.lemmatize("books")

[lemmatizer.lemmatize(token) for token in token_list]

lemma_word = []
for token in token_list:
    lemma_word.append(lemmatizer.lemmatize(token))
lemma_word

join_text = ' '.join(lemma_word)
join_text

def data_cleaner(data):
    clean_data = []
    for sentence in tqdm(data):
        cleantext = BeautifulSoup(sentence, "lxml").text 
        cleantext = re.sub(r'[^\w\s]','',cleantext) 
        cleantext = [token for token in cleantext.lower().split() if token not in stopword] 
        clean_text = ' '.join([lemmatizer.lemmatize(token) for token in cleantext])
        clean_data.append(clean_text.strip())
    return clean_data

clean_data = data_cleaner(data.Comment.values)

clean_data[100]

"""train test split"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data, data.Emotion, test_size=0.2, random_state=42 ,stratify=data["Emotion"])

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_train = le.fit_transform(y_train)

le1 = LabelEncoder()
y_test = le1.fit_transform(y_test)

label_encoder = LabelEncoder()
data['Emotion']= label_encoder.fit_transform(data['Emotion'])
  
data['Emotion'].unique()

print(X_train.shape,y_train.shape)
print(X_test.shape,y_test.shape)

clean_data_train_data = data_cleaner(X_train.Comment.values)

X_train = X_train.reset_index(drop=True)
X_train['cleaned_text'] = clean_data_train_data
X_train.head()

clean_data_test_data =  data_cleaner(X_test.Comment.values)

X_test = X_test.reset_index(drop=True)
X_test['cleaned_text'] = clean_data_test_data
X_test.head()

"""Vectorizer"""

vec = CountVectorizer()
vec.fit(X_train.cleaned_text)
train_x_bow = vec.transform(X_train.cleaned_text)
test_x_bow = vec.transform(X_test.cleaned_text)

print(train_x_bow.shape)
print(test_x_bow.shape)

"""Naive Bayes with Hyperparameter tuning"""

classfier = MultinomialNB()

alpha_ranges = {"alpha":[10**-2,10**-1,10**0,10**1,10**2]}
grid_search =GridSearchCV(classfier , param_grid = alpha_ranges ,scoring= "accuracy",cv=3,return_train_score = True)
grid_search.fit(train_x_bow,y_train)

alpha = [10**-2,10**-1,10**0,10**1,10**2]
train_acc = grid_search.cv_results_['mean_train_score']
train_std = grid_search.cv_results_['std_train_score']
test_acc = grid_search.cv_results_['mean_test_score']
test_std = grid_search.cv_results_['std_test_score']

"""Plot mean accuracy scores for training and testing scores"""

plt.plot(alpha, train_acc,
     label = "Training Score", color = 'b')
plt.plot(alpha, test_acc,
   label = "Cross Validation Score", color = 'r')
plt.title("Validation Curve with Naive Bayes Classifier")
plt.xlabel("alpha")
plt.ylabel("Accuracy")
plt.tight_layout()
plt.legend(loc = 'best')
plt.show()

grid_search.best_estimator_

classifier = MultinomialNB(alpha=1)
classifier.fit(train_x_bow,y_train)

predict = classifier.predict(test_x_bow)

print("accuracy is :",accuracy_score(y_test,predict))

print("accuracy is:",classification_report(y_test,predict))

text = [
            "He was speechles when he found out he was accepted to this new job",
            
        ]
    
text_vec = vec.transform(text)
classifier.predict(text_vec)

result = le.inverse_transform([2])
print(f"{text}: {result}\n\n")

text = [
            "I feel like im all alone in this world",
            
        ]
    
text_vec = vec.transform(text)
classifier.predict(text_vec)

result = le.inverse_transform([4])
print(f"{text}: {result}\n\n")

from joblib import dump
dump(vec,"vectors.joblib")
dump(classifier,"model.joblib")

!pip install fastapi nest-asyncio pyngrok uvicorn

from matplotlib.mathtext import List
from typing import Optional
from joblib import load
from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
app = FastAPI()


app.add_middleware(
    CORSMiddleware,
    allow_origins=['*'],
    allow_credentials=True,
    allow_methods=['*'],
    allow_headers=['*'],
)

vector = load("vectors.joblib")
model = load("model.joblib")

class get_Comment(BaseModel):
    Comment :str

@app.get("/")
def read_root():
    return {"Hello": "World"}


@app.post("/prediction")
def predict_emotion(payload:get_Comment):

  print(payload)

  text = payload.Comment
  vec = vector.transform([text])
  print(text)
  print(vec)
  prediction = model.predict(vec)
  prediction = int(prediction)

  emotions = {0: "anger", 1: "fear", 2: "joy", 3: "love", 4: "sadness", 5: "surprise"}
  return{"message": text , "prediction": emotions[prediction]}

import nest_asyncio
from pyngrok import ngrok
import uvicorn

ngrok_tunnel = ngrok.connect(8000)
print('Public URL:', ngrok_tunnel.public_url)
print(f"""
RUN THE FOLLOWING COMMAND TO MAKE A GET REQUEST

curl -XPOST {ngrok_tunnel.public_url}/prediction 


""")
nest_asyncio.apply()
uvicorn.run(app, port=8000)